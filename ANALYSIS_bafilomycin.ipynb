{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Cxcr7 Response to Bafilomycin Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- **Experiment & Data**\n",
    "    - Scope: 880 Airy-SR\n",
    "    - Structure: neuromasts\n",
    "    - Genetic background: wild-type\n",
    "    - Perturbation1: Bafilomcin treatment (inhibits lysosomal degradation)\n",
    "    - Perturbation2: Heat-shock expression of Sdf1a\n",
    "    - Markers:\n",
    "        - BAC(Cxcr7) expressing WT Cxcr7\n",
    "        - Red membrane marker (Lyn:Ruby) as counterlabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "# Generic\n",
    "from __future__ import division\n",
    "import os, sys\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import scipy.ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Specific\n",
    "import re, pickle\n",
    "from ipywidgets import interact\n",
    "from util.widget_helpers import savebutton\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings\n",
    "\n",
    "# Path of dir containing data\n",
    "fpath = 'data_full'\n",
    "\n",
    "# Ordered list of experiments to load\n",
    "labels     = ['WT_20170902',\n",
    "              'baf_20170902',\n",
    "              'hs_20170902',\n",
    "              'bafANDhs_20170902',\n",
    "              'WT_20170903',\n",
    "              'baf_20170903',\n",
    "              'hs_20170903',\n",
    "              'bafANDhs_20170903',\n",
    "              'p1WT_20180210',\n",
    "              'p1baf_20180210',\n",
    "              'p2WT_20180210',\n",
    "              'p2baf_20180210',\n",
    "              'p2hs_20180210',\n",
    "              'p2bafANDhs_20180210',\n",
    "              'p3WT_20180211',\n",
    "              'p3baf_20180211',\n",
    "              'p3hs_20180211',\n",
    "              'p3bafANDhs_20180211']\n",
    "\n",
    "# Dates to exclude\n",
    "exclude_dates = []\n",
    "\n",
    "# Dict of colors to represent the different experiments  \n",
    "color_dict = {'WT_20170902'       : 'lightskyblue',\n",
    "              'baf_20170902'      : 'limegreen',\n",
    "              'hs_20170902'       : 'tomato',\n",
    "              'bafANDhs_20170902' : 'orange',\n",
    "              'WT_20170903'       : 'lightskyblue',\n",
    "              'baf_20170903'      : 'limegreen',\n",
    "              'hs_20170903'       : 'tomato',\n",
    "              'bafANDhs_20170903' : 'orange',\n",
    "              'p1WT_20180210'     : 'lightskyblue',\n",
    "              'p1baf_20180210'    : 'limegreen',\n",
    "              'p2WT_20180210'     : 'lightskyblue',\n",
    "              'p2baf_20180210'    : 'limegreen',\n",
    "              'p2hs_20180210'     : 'tomato',\n",
    "              'p2bafANDhs_20180210' : 'orange',\n",
    "              'p3WT_20180211'     : 'lightskyblue',\n",
    "              'p3baf_20180211'    : 'limegreen',\n",
    "              'p3hs_20180211'     : 'tomato',\n",
    "              'p3bafANDhs_20180211' : 'orange'}\n",
    "\n",
    "# Ordered list of experiment sets for grouping\n",
    "experiments = ['WT', 'HS', 'BAF', 'BAF+HS']\n",
    "\n",
    "# Dict merging experiments into sets\n",
    "experiment_dict = {'WT_20170902'       : 'WT',\n",
    "                   'baf_20170902'      : 'BAF',\n",
    "                   'hs_20170902'       : 'HS',\n",
    "                   'bafANDhs_20170902' : 'BAF+HS',\n",
    "                   'WT_20170903'       : 'WT',\n",
    "                   'baf_20170903'      : 'BAF',\n",
    "                   'hs_20170903'       : 'HS',\n",
    "                   'bafANDhs_20170903' : 'BAF+HS',\n",
    "                   'p1WT_20180210'     : 'WT',\n",
    "                   'p1baf_20180210'    : 'BAF',\n",
    "                   'p2WT_20180210'     : 'WT',\n",
    "                   'p2baf_20180210'    : 'BAF',\n",
    "                   'p2hs_20180210'     : 'HS',\n",
    "                   'p2bafANDhs_20180210' : 'BAF+HS',\n",
    "                   'p3WT_20180211'     : 'WT',\n",
    "                   'p3baf_20180211'    : 'BAF',\n",
    "                   'p3hs_20180211'     : 'HS',\n",
    "                   'p3bafANDhs_20180211' : 'BAF+HS'}\n",
    "\n",
    "# Dict of colors for experiment sets\n",
    "experiment_color_dict = {'WT'     : 'lightskyblue',\n",
    "                         'BAF'    : 'limegreen',\n",
    "                         'HS'     : 'tomato',\n",
    "                         'BAF+HS' : 'orange'}\n",
    "\n",
    "# Dict matching experiments with controls\n",
    "control_dict = {'WT_20170902'       : 'WT_20170902',\n",
    "                'baf_20170902'      : 'WT_20170902',\n",
    "                'hs_20170902'       : 'WT_20170902',\n",
    "                'bafANDhs_20170902' : 'WT_20170902',\n",
    "                'WT_20170903'       : 'WT_20170903',\n",
    "                'baf_20170903'      : 'WT_20170903',\n",
    "                'hs_20170903'       : 'WT_20170903',\n",
    "                'bafANDhs_20170903' : 'WT_20170903',\n",
    "                'p1WT_20180210'     : 'p1WT_20180210',\n",
    "                'p1baf_20180210'    : 'p1WT_20180210',\n",
    "                'p2WT_20180210'     : 'p2WT_20180210',\n",
    "                'p2baf_20180210'    : 'p2WT_20180210',\n",
    "                'p2hs_20180210'     : 'p2WT_20180210',\n",
    "                'p2bafANDhs_20180210' : 'p2WT_20180210',\n",
    "                'p3WT_20180211'     : 'p3WT_20180211',\n",
    "                'p3baf_20180211'    : 'p3WT_20180211',\n",
    "                'p3hs_20180211'     : 'p3WT_20180211',\n",
    "                'p3bafANDhs_20180211' : 'p3WT_20180211'}\n",
    "\n",
    "# Dict matching experiment sets with controls\n",
    "experiment_control_dict = {'WT'     : 'WT',\n",
    "                           'BAF'    : 'WT',\n",
    "                           'HS'     : 'WT',\n",
    "                           'BAF+HS' : 'WT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Loading\n",
    "\n",
    "# Elaborate data loading & parsing function\n",
    "def load_data(fpath):\n",
    "\n",
    "    # Get filenames\n",
    "    fnames = [fname for fname in os.listdir(fpath) if fname.endswith('masked_LMs.npz')]\n",
    "\n",
    "    # Extract experiment labels and dates\n",
    "    label_pattern = re.compile(r'\\(([A-Za-z0-9_]+)\\)')\n",
    "    data_labels   = [re.search(label_pattern, fname).group(1) for fname in fnames]\n",
    "    date_pattern  = re.compile(r'_([0-9]{8})\\)')\n",
    "    data_dates    = [re.search(date_pattern, fname).group(1) for fname in fnames]\n",
    "    \n",
    "    # Keep only desired experiments\n",
    "    fnames      = [fname for fname,label,date in zip(fnames,data_labels,data_dates) \n",
    "                   if label in labels and not date in exclude_dates]\n",
    "    data_labels = [label for label,date in zip(data_labels,data_dates)\n",
    "                   if label in labels and not date in exclude_dates]\n",
    "    data_dates  = [date for label,date  in zip(data_labels,data_dates)\n",
    "                   if label in labels and not date in exclude_dates]\n",
    "    \n",
    "    # Sort according to experiment list\n",
    "    sort_indices = [labels.index(label) for label in data_labels]  # Get indices\n",
    "    data_labels  = [label for index,label in sorted(zip(sort_indices, data_labels))]  # Sort the labels\n",
    "    data_dates   = [date  for index,date  in sorted(zip(sort_indices, data_dates))]   # Sort the dates\n",
    "    fnames       = [fname for index,fname in sorted(zip(sort_indices, fnames))]       # Sort the filenames  \n",
    "    \n",
    "    # Metadata prep: load data\n",
    "    metadata = []\n",
    "    with open(os.path.join(fpath, r\"metadata.txt\"), \"r\") as infile:\n",
    "        for line in infile.readlines():\n",
    "            metadata.append(line.strip().split('\\t'))\n",
    "\n",
    "    # Metadata prep: int conversion function\n",
    "    def cint(astr):\n",
    "        try: return int(astr)\n",
    "        except ValueError: return np.nan            \n",
    "            \n",
    "    # Load the data\n",
    "    data_masked = []\n",
    "    data_memsub = []\n",
    "    data_intens = []\n",
    "    data_meta   = []\n",
    "    for fname in fnames:\n",
    "\n",
    "        # Load masked dataset\n",
    "        masked_LMs = np.load(os.path.join(fpath, fname))\n",
    "        data_masked.append(masked_LMs)\n",
    "\n",
    "        # Load memsubbed dataset\n",
    "        memsub_LMs = np.load(os.path.join(fpath, fname[:-8]+\"_memsub_LMs.npz\"))\n",
    "        data_memsub.append(memsub_LMs)\n",
    "        \n",
    "        # Load intensity values\n",
    "        with open(os.path.join(fpath, fname[:-14]+\"measurements.pkl\"),\"rb\") as infile:\n",
    "            data_intensity = pickle.load(infile)\n",
    "        data_intens.append(data_intensity)\n",
    "        \n",
    "        # Metadata: load corresponding values\n",
    "        for line in metadata:\n",
    "            if line[0]==fname[:-20]:\n",
    "                data_meta.append([cint(line[5]),cint(line[6])])\n",
    "                    \n",
    "    # Get the number of landmarks\n",
    "    num_lms = data_masked[0]['lm_cx7'].shape[0]\n",
    "        \n",
    "    # Unfold the .npz structure\n",
    "    data = {}\n",
    "    for key in data_masked[0].files:\n",
    "        data[key] = np.array([d[key] for d in data_masked])\n",
    "    for key in data_memsub[0].files:\n",
    "        data[key] = np.array([d[key] for d in data_memsub])\n",
    "        \n",
    "    # Unfold the intensity dicts\n",
    "    for key in data_intens[0].keys():\n",
    "        data['intensity_'+key] = np.array([d[key] for d in data_intens])\n",
    "        \n",
    "    # Unfold the metadata\n",
    "    for i,key in enumerate(['meta_time','meta_nm']):\n",
    "        data[key] = np.array([d[i] for d in data_meta])\n",
    "    \n",
    "    # Return results\n",
    "    return np.array(data_labels), num_lms, data\n",
    "\n",
    "# Call data loading func\n",
    "data_labels, num_lms, data = load_data(fpath)\n",
    "\n",
    "# Get sorted data keys\n",
    "data_keys = sorted(data.keys())\n",
    "\n",
    "# Remove labels of excluded dates\n",
    "labels = [label for label in labels if not label[-8:] in exclude_dates]\n",
    "\n",
    "# Get experiment labels\n",
    "data_experiments = np.array([experiment_dict[label] for label in data_labels])\n",
    "\n",
    "# Report\n",
    "print \"\\nLoaded\", len(data_labels), \"datasets with\", num_lms, \"landmarks per set.\"\n",
    "print \"\\nSamples per condition:\"\n",
    "for label in labels: print \"  {0:16}{1:>4}\".format(label, np.sum(data_labels==label))\n",
    "print \"\\nSamples per experiment:\"\n",
    "for experiment in experiments: print \"  {0:2}{1:>4}\".format(experiment, np.sum(data_experiments==experiment))\n",
    "print \"\\nAvailable measurements are:\"\n",
    "for key in data_keys: print \"  {0:<21}{1}\".format(key, data[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Cloud Distribution Analysis: Apical Distance Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apical_distance_hist(d, label, min_sampling, color_dict, bins=25,\n",
    "                         alpha=0.5, xlabel='', ylabel=''):\n",
    "\n",
    "    # Subsample to get everything to the same number of LMs\n",
    "    min_sample_nr  = num_lms * np.min(np.unique(min_sampling, return_counts=True)[1])\n",
    "    d = d[np.random.choice(np.arange(d.shape[0]),\n",
    "                           size=min_sample_nr, replace=False)]    \n",
    "    \n",
    "    # Get color\n",
    "    c = color_dict[label]\n",
    "\n",
    "    # Make hist\n",
    "    plt.hist(d, bins=np.linspace(0, np.max(d), bins), \n",
    "             histtype='stepfilled', color=c, edgecolor=None, alpha=alpha, \n",
    "             label=label)\n",
    "    plt.hist(d, bins=np.linspace(0, np.max(d), bins), \n",
    "             histtype='step', color=c, alpha=1)\n",
    "    \n",
    "    # Labeling\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive Apical Distance Histogram\n",
    "\n",
    "# Initiate\n",
    "@interact(val=['memsub_lum_dist_cx7']+[v for v in data_keys if 'lum_dist' in v \n",
    "                                       and not v=='memsub_lum_dist_cx7'],\n",
    "          show_all=False)\n",
    "#@savebutton\n",
    "def interactive_hist(val='memsub_lum_dist_cx7',\n",
    "                     show_all=False):\n",
    "    \n",
    "    # Prep plot\n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    # Make the summarized plots (per experiment)\n",
    "    if not show_all:\n",
    "        for experiment in experiments:\n",
    "            plot_labels = [label for label in labels\n",
    "                           if experiment_dict[label]==experiment]\n",
    "            data_to_plot = data[val][np.in1d(data_labels,plot_labels)].flatten()\n",
    "            apical_distance_hist(data_to_plot, experiment, \n",
    "                                 data_experiments, experiment_color_dict,\n",
    "                                 alpha=0.2)\n",
    "    \n",
    "    # Make the individual plots (per label)\n",
    "    else:\n",
    "        for label in labels:\n",
    "            data_to_plot = data[val][data_labels==label].flatten()\n",
    "            apical_distance_hist(data_to_plot, label, \n",
    "                                 data_labels, color_dict,\n",
    "                                 alpha=0.1)            \n",
    "    \n",
    "    # Global plot cosmetics\n",
    "    plt.title(\"Apical Distance Histogram [\"+val+\"]\")\n",
    "    plt.xlabel(\"Distance from Lumen $[\\mu m]$\")\n",
    "    plt.ylabel(\"Number of Landmarks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Cloud Distribution Analysis: Registered Overlays (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def registered_overlay_scatter(d, label, fig, ax, min_sampling, ylbl='y', clbl='z',\n",
    "                               xlim=[-50,50], ylim=[-20,20], vlim=[-0.5,1.5]):\n",
    "\n",
    "    # Subsample to get everything to the same number of LMs\n",
    "    min_sample_nr  = num_lms * np.min(np.unique(min_sampling, return_counts=True)[1])\n",
    "    d = d[np.random.choice(np.arange(d.shape[0]),\n",
    "                           size=min_sample_nr, replace=False)]     \n",
    "\n",
    "    # Make scatter plot\n",
    "    scat = ax.scatter(d[:,2], d[:,1], label=label,\n",
    "                      c=d[:,0], cmap='inferno', vmin=vlim[0], vmax=vlim[1],\n",
    "                      s=5, edgecolor='', alpha=0.5)\n",
    "\n",
    "    # Colorbar\n",
    "    cbar = fig.colorbar(scat, ax=ax)\n",
    "    cbar.set_label(clbl, rotation=270)\n",
    "\n",
    "    # Set limits\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Label\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(ylbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive Landmark Overlays: XY\n",
    "\n",
    "# Initiate\n",
    "@interact(val=['lm_memsub_cx7_tf']+[v for v in data_keys if not 'lum_dist' in v \n",
    "                                    and 'tf' in v and not v=='lm_memsub_cx7_tf'])\n",
    "#@savebutton\n",
    "def interactive_hist(val='lm_memsub_cx7_tf'):\n",
    "    \n",
    "    # Prep plot\n",
    "    fig, ax = plt.subplots(len(experiments), figsize=(10,3*len(experiments)),\n",
    "                           sharex=True, sharey=True)\n",
    "    \n",
    "    # Make the plots\n",
    "    for experiment,axis in zip(experiments, ax):\n",
    "        plot_labels = [label for label in labels\n",
    "                       if experiment_dict[label]==experiment]\n",
    "        data_to_plot = data[val][np.in1d(data_labels,plot_labels)]\n",
    "        data_to_plot = np.concatenate(data_to_plot)\n",
    "        registered_overlay_scatter(data_to_plot, experiment, fig, axis, data_experiments,\n",
    "                                   ylbl='y', clbl='z', xlim=[-50,50], ylim=[-20,20], vlim=[-0.5,1.5])\n",
    "        \n",
    "    # Global plot cosmetics\n",
    "    plt.xlabel('x')\n",
    "    plt.suptitle('Registered Landmark Distributions (x,y) of '+val, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive Landmark Overlays: XZ\n",
    "\n",
    "# Initiate\n",
    "@interact(val=['lm_memsub_cx7_tf']+[v for v in data_keys if not 'lum_dist' in v \n",
    "                                    and 'tf' in v and not v=='lm_memsub_cx7_tf'])\n",
    "#@savebutton\n",
    "def interactive_hist(val='lm_memsub_cx7_tf'):\n",
    "    \n",
    "    # Prep plot\n",
    "    fig, ax = plt.subplots(len(experiments), figsize=(10,3*len(experiments)),\n",
    "                           sharex=True, sharey=True)\n",
    "    \n",
    "    # Make the plots\n",
    "    for experiment,axis in zip(experiments, ax):\n",
    "        plot_labels = [label for label in labels\n",
    "                       if experiment_dict[label]==experiment]\n",
    "        data_to_plot = data[val][np.in1d(data_labels,plot_labels)]\n",
    "        data_to_plot = np.concatenate(data_to_plot)\n",
    "        data_to_plot = data_to_plot[:,[1,0,2]]  # Changing the axis\n",
    "        registered_overlay_scatter(data_to_plot, experiment, fig, axis, data_experiments,\n",
    "                                   ylbl='z', clbl='y', xlim=[-50,50], ylim=[-0.5,1.5], vlim=[-20,20])\n",
    "        \n",
    "    # Global plot cosmetics\n",
    "    plt.xlabel('x')\n",
    "    plt.suptitle('Registered Landmark Distributions (x,z) of '+val, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity Analysis: Absolute Intensity Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intensity_absolute_boxplot(d, val, is_normed=False, pool=False):\n",
    "\n",
    "    # Pool by experiments\n",
    "    # Note: This is a bit hamfisted but it works for now.\n",
    "    if pool:\n",
    "        exp_d = []\n",
    "        for exp in experiments:\n",
    "            exp_d.append([])\n",
    "            for dset, lbl in zip(d, labels):\n",
    "                if experiment_dict[lbl] == exp:\n",
    "                    exp_d[-1].append(dset)\n",
    "            exp_d[-1] = np.concatenate(exp_d[-1])\n",
    "        d = exp_d\n",
    "        lbls  = experiments\n",
    "        cdict = experiment_color_dict\n",
    "        ctrldict = experiment_control_dict\n",
    "    else:\n",
    "        lbls  = labels\n",
    "        cdict = color_dict\n",
    "        ctrldict = control_dict\n",
    "\n",
    "    # Compute and print p-values\n",
    "    print \"\\np-values:\"\n",
    "    ctrl_idx = [lbls.index(ctrldict[label]) for label in lbls]\n",
    "    p = [mannwhitneyu(d[i], d[ctrl_idx[i]], alternative='two-sided')[1] \n",
    "         if ((np.std(d[i])>0) and (np.std(d[ctrl_idx[i]])>0)) else 'NOT ENOUGH DATA'\n",
    "         for i,label in enumerate(lbls)]\n",
    "    for i, label in enumerate(lbls):\n",
    "        print label, '\\t', p[i]\n",
    "    \n",
    "    # Prep\n",
    "    fig, ax = plt.subplots(1, figsize=(1.5*len(lbls),4))\n",
    "\n",
    "    # Create boxplot\n",
    "    bp = ax.boxplot(d, showfliers=False)\n",
    "\n",
    "    # Boxplot cosmetics\n",
    "    for item in ['boxes', 'whiskers']:\n",
    "        plt.setp(bp[item], color='k')\n",
    "    ax.set_xticklabels([lbl+\" (N=\"+str(len(d[idx]))+\")\" \n",
    "                        for idx,lbl in enumerate(lbls)])\n",
    "    fig.autofmt_xdate()\n",
    "    if is_normed:\n",
    "        ax.set_title(\"Ctrl-Normed Mean Intensity: \"+val[10:])\n",
    "    else:\n",
    "        ax.set_title(\"Mean Intensity: \"+val[10:])\n",
    "        \n",
    "    # Add jittered data\n",
    "    for i,label in enumerate(lbls):\n",
    "        y = d[i]                                      # Values\n",
    "        x = np.random.normal(i+1, 0.04, size=len(y))  # Jitter\n",
    "        ax.scatter(x, y, c=cdict[label], \n",
    "                   alpha=0.7, edgecolor='', s=30)\n",
    "\n",
    "    # Axis cosmetics\n",
    "    ax.set_ylim([ax.get_ylim()[0] - ax.get_ylim()[0] * 0.1, \n",
    "                 ax.get_ylim()[1] + ax.get_ylim()[1] * 0.1])\n",
    "    if is_normed:\n",
    "        ax.set_ylabel(\"Ctrl-Normed Mean Intensity\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"Mean Intensity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive Absolute Intensity Boxplot\n",
    "\n",
    "# Initiate\n",
    "@interact(val=['intensity_all_cx7']+[v for v in data_keys if 'intensity' in v\n",
    "                                     and not v=='intensity_all_cx7'],\n",
    "          ctrl_norm=True, pool=True)\n",
    "#@savebutton\n",
    "def interactive_rbp(val='intensity_all_cx7', \n",
    "                    ctrl_norm=True, pool=True):\n",
    "    \n",
    "    # Get data\n",
    "    d = [data[val][data_labels==label] for label in labels]\n",
    "    \n",
    "    # Normalize by mean of respective control\n",
    "    if ctrl_norm:\n",
    "        ctrl_idx = [labels.index(control_dict[label]) for label in labels]\n",
    "        d = [d[i] / np.mean(d[ctrl_idx[i]]) for i,label in enumerate(labels)]\n",
    "    \n",
    "    # Make the plot\n",
    "    intensity_absolute_boxplot(d, val, is_normed=ctrl_norm, pool=pool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "427e3982f20b4f69a338346fe9377735": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "7745a1e0de6f4727b1d0932b9a4b60fc": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "b1594c9a7c3244acba9db8f4ad63ac28": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "c209e3e852164e769e8867a1039c772b": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "cc2e64476be640ad8d2c63d77068b122": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
